{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sprint13　DNN\n",
    "## スクラッチでDNNを実装し、MNISTの手書き文字認識に適用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnistの手書き文字認識問題\n",
    "\n",
    "784ピクセルの画像データから数字（0-9）を分類する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "X = train_df.drop('label', axis=1)\n",
    "y = pd.get_dummies(train_df['label'])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "# 訓練とテストデータに分割\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出力関数　ソフトマックス関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コスト関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, y_pred):\n",
    "    \n",
    "    data_size = y.shape[0]\n",
    "\n",
    "    # クロスエントロピー誤差関数　y_predは０になりえるので -inf にならないためにすごく小さい補正値を入れる\n",
    "    cross_entorpy = -np.sum(y * np.log(y_pred + 1e-7))\n",
    "    \n",
    "    error = cross_entorpy  / data_size\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 損失関数\n",
    "        self.y = None       # softmaxの出力\n",
    "        self.t = None       # 教師データ（one-hot vector)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size # delta3に相当\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### アフィン変換層\n",
    "\n",
    "更新手法をsgd,adagrad,adamと切り替えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b, params= {}):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        # パラメータの微分値\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        # 更新式のスイッチング\n",
    "        self.lr = params['lr']\n",
    "        \n",
    "        # optimizeメソッドを\n",
    "        if params['optimizer']=='sgd':\n",
    "            self.optimize = self.update_sgd\n",
    "        elif params['optimizer'] == 'adagrad':\n",
    "            self.h = np.zeros_like(W)\n",
    "            self.optimize = self.update_adagrad\n",
    "        else: # params['optimizer'] == 'adam':\n",
    "            self.m = np.zeros_like(W)\n",
    "            self.v = np.zeros_like(W)\n",
    "            self.beta1 = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.optimize = self.update_adam\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    def update_sgd(self):\n",
    "        self.W -= self.lr * self.dW\n",
    "        self.b -= self.lr * self.db\n",
    "    \n",
    "    # adagrad 少しずつ更新量が減っていく\n",
    "    def update_adagrad(self, lr = 0.01):\n",
    "        self.h += self.dW ** 2\n",
    "        self.W -= self.lr * self.dW / (np.sqrt(self.h) + 1e-7)\n",
    "        self.b -= self.lr * self.db\n",
    "        \n",
    "    \n",
    "    def update_adam(self, lr = 0.01):\n",
    "        self.m = self.beta1 * self.m + (1- self.beta1) * self.dW\n",
    "        self.v = self.beta2 * self.v + (1- self.beta2) * (self.dW * self.dW)\n",
    "        \n",
    "        m_hat = self.m / (1 - self.beta1)\n",
    "        v_hat = self.v / (1 - self.beta2)\n",
    "        \n",
    "        self.W -= self.lr * m_hat / (np.sqrt(v_hat) + 1e-8)\n",
    "        self.b -= self.lr * self.db\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数層\n",
    "\n",
    "ReLU、tanh,シグモイド関数を切り替えて使用できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    '''\n",
    "    活性化関数を設定できる \n",
    "    'tanh'\n",
    "    'sigmoid'\n",
    "    'relu'\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        self.out = None\n",
    "        self.mask = None\n",
    "        # optimizeメソッドを\n",
    "        if params['act_func']=='tanh':\n",
    "            self.forward = self.forward_tanh\n",
    "            self.backward = self.backward_tanh\n",
    "        elif params['act_func'] == 'sigmoid':\n",
    "            self.forward = self.forward_sigmoid\n",
    "            self.backward = self.backward_sigmoid\n",
    "        else: # params['act_func'] == 'relu':\n",
    "            self.forward = self.forward_relu\n",
    "            self.backward = self.backward_relu\n",
    "     \n",
    "    def forward_relu(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward_relu(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "\n",
    "    # tanh \n",
    "    def forward_tanh(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward_tanh(self, dout):\n",
    "        dx = dout * (1 - np.tanh(dout)**2)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    # sigmoid関数\n",
    "    def forward_sigmoid(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward_sigmoid(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチノーマライゼーション層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "    def __init__(self, params):\n",
    "        self.out = None\n",
    "        self.beta = 0.0\n",
    "        self.gamma = 1.0\n",
    "        self.lr = params['lr']\n",
    "        self.eps = 1e-8\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        data_size, input_size = x.shape\n",
    "        \n",
    "        # step1: 平均を求める\n",
    "        mu = np.mean(x, axis=0)\n",
    "        \n",
    "        # step2: 偏差\n",
    "        self.xmu = x - mu\n",
    "        \n",
    "        # step3 : 偏差の２乗\n",
    "        sq = self.xmu ** 2\n",
    "        \n",
    "        # step4 : 分散を求める\n",
    "        self.var = np.var(x, axis=0)\n",
    "        \n",
    "        # step5 : 分散のルートを取った値を求める\n",
    "        self.sqrtvar = np.sqrt(self.var + self.eps)\n",
    "        \n",
    "        # step6 : sqrtvarの逆数（invert）\n",
    "        self.ivar = 1.0/ self.sqrtvar\n",
    "        \n",
    "        # step7 : 標準化した値\n",
    "        self.xhat = self.xmu * self.ivar\n",
    "        \n",
    "        # step8\n",
    "        gammax = self.gamma * self.xhat\n",
    "        \n",
    "        # step9\n",
    "        out = gammax + self.beta\n",
    "        \n",
    "        return out\n",
    "\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        #get the dimensions of the input/output\n",
    "        N, D = dout.shape\n",
    "        \n",
    "        # step9\n",
    "        self.d_beta = np.sum(dout, axis=0)\n",
    "        dgammax = dout\n",
    "        \n",
    "        # step8\n",
    "        self.d_gamma = np.sum(dgammax*self.xhat, axis=0)\n",
    "        dxhat = self.d_gamma * self.gamma\n",
    "        \n",
    "        # step7\n",
    "        divar = np.sum(dxhat*self.xmu, axis=0)\n",
    "        dxmu1 = dxhat * self.ivar\n",
    "        \n",
    "        # step6\n",
    "        dsqrtvar = -1. /(self.sqrtvar**2) * divar\n",
    "        \n",
    "        # step5\n",
    "        dvar = 0.5 * 1. / np.sqrt(self.var+self.eps) * dsqrtvar\n",
    "        \n",
    "        # step4\n",
    "        dsq = 1. / N * np.ones((N, D)) * dvar\n",
    "        \n",
    "        # step3\n",
    "        dxmu2 = 2 * self.xmu * dsq\n",
    "        \n",
    "        # step2\n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "        dmu = -1 * np.sum(dxmu1 + dxmu2, axis=0)\n",
    "        \n",
    "        # step1\n",
    "        dx2 = 1. / N * np.ones((N, D)) * dmu\n",
    "        \n",
    "        # step0\n",
    "        dx = dx1 + dx2\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    \n",
    "    def optimize(self):\n",
    "        self.gamma -= self.lr * self.dgamma\n",
    "        self.beta -= self.lr * self.dbeta\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ドロップアウト層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg :\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1 - self.dropout_ratio)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネットワーククラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Layers:\n",
    "    def __init__(self, params):\n",
    "        unit_size_list = [params['input_size']]\n",
    "        unit_size_list.extend(params['hidden_layer_list'])\n",
    "        unit_size_list.append(params['output_size'])\n",
    "        \n",
    "        self.params = {}\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        \n",
    "        for i in range(1, len(unit_size_list)):\n",
    "            # 重みの初期化（gauss, xavier, He を選択できる）\n",
    "            init_W = np.random.randn(unit_size_list[i-1], unit_size_list[i])\n",
    "            init_b = np.zeros([1, unit_size_list[i]])\n",
    "            if params['init'] == 'gauss':\n",
    "                init_W *= 0.01\n",
    "            elif params['init'] == 'xavier':\n",
    "                init_W /= np.sqrt(unit_size_list[i-1])\n",
    "            else: # He\n",
    "                init_W = init_W / np.sqrt(unit_size_list[i-1]) * np.sqrt(2) \n",
    "                \n",
    "            # アフィン変換層（Wx + b）を追加する\n",
    "            self.layers['Affine' + str(i)] = Affine(init_W, init_b, params)\n",
    "            \n",
    "            # 最終層以外はバッチノーマリゼーション層と活性化関数層を追加する\n",
    "            if i < (len(unit_size_list)-1):\n",
    "                if params['batch_norm'] == True:\n",
    "                    self.layers['BatchNorm' + str(i)] = BatchNorm(params)\n",
    "                self.layers['Active' + str(i)] = Activation(params)\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        # forwardを繰り返す\n",
    "        # ソフトマックスを通さなくても答えは出るのでこれで予測とする \n",
    "        # argmaxでラベルを取れる\n",
    "        for layer in self.layers.values():\n",
    "            x =layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        # 正答率を小数点第二桁で出力する\n",
    "        y_pred = self.predict(x)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(t, axis=1)\n",
    "        data_size = x.shape[0]\n",
    "\n",
    "        correct_count = np.sum([y_true == y_pred]) \n",
    "        score = correct_count / data_size * 100\n",
    "        return round(score, 2)\n",
    "    \n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.layers['Affine1'].W)\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.layers['Affine1'].b)\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.layers['Affine2'].W)\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.layers['Affine2'].b)\n",
    "        \n",
    "        print(\"Numerical: {}\".format(grads['W1']) )\n",
    "        print(\"OreOre   : {}\".format(self.layers['Affine1'].W))\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    def optimize(self, x, t):\n",
    "        \n",
    "        # forward \n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.lastLayer.backward(1)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # optimizeメソッドがある層は更新を行う\n",
    "        # AffineとBatchNorm層のみ行う\n",
    "        for layer in self.layers.values():\n",
    "            if hasattr(layer, \"optimize\"):\n",
    "                layer.optimize()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, init='gauss', iteration = 500, lr = 0.05,  lam = 0.01, \n",
    "                 batch_mode = 'mini', act_func='relu',\n",
    "                 batch_size_rate = 0.1, hidden_layer_list = [5], optimizer='sgd',\n",
    "                 batch_norm=False, drop_out=False):\n",
    "        \"\"\" ハイパーパラメータ解説\n",
    "        init: 初期化方法\n",
    "            'he' : \n",
    "            'gauss' \n",
    "            'xavier'\n",
    "        lr : 学習率\n",
    "        lam : 正則化項の率\n",
    "        batch_size: バッチサイズ\n",
    "            'batch' : フルサイズ\n",
    "            'mini' 0< x< 1: フルサイズ割合 0.1なら全体の0.1サイズ使用する\n",
    "            'online' : オンライン学習　１データのみ\n",
    "        hidden_layer_list : 隠れ層のリスト、層のユニットをリストで入力　例[2, 3]　ユニット数２、\n",
    "                            ユニット数３の隠れ層\n",
    "        optimizer : 勾配の更新手法\n",
    "            'sgd' : 確率的勾配降下法\n",
    "            'adam': \n",
    "            'adagrad':\n",
    "        act_func: 活性化関数の名前　パラメータ名が微妙\n",
    "            'relu' : ReLU関数\n",
    "            'tanh' : tanh\n",
    "            'sigmoid' : シグモイド関数\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['iteration'] = iteration\n",
    "        self.params['init'] = init\n",
    "        self.params['lr'] = lr\n",
    "        self.params['lam'] = lam\n",
    "        self.params['batch_mode'] = batch_mode # データ数が決まったらそれに基づいて変更する\n",
    "        self.params['batch_size_rate'] = batch_size_rate # ミニバッチ法のときのみ使用する\n",
    "        self.params['hidden_layer_list'] = hidden_layer_list\n",
    "        self.params['optimizer'] = optimizer\n",
    "        self.params['batch_norm'] = batch_norm\n",
    "        self.params['drop_out'] = drop_out\n",
    "        self.params['act_func'] = act_func\n",
    "        \n",
    "    def train(self, X, y, params={}):\n",
    "        # 入力パラメータがあれば更新する\n",
    "        for key in params:\n",
    "                self.params[key] = params[key]\n",
    "        \n",
    "        # 正規化　必要？\n",
    "        X = X / 255.0\n",
    "        \n",
    "        # 訓練とテストデータに分割\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        self.params['data_size'] = X_train.shape[0]\n",
    "        self.params['input_size'] = X_train.shape[1]\n",
    "        self.params['output_size'] = y_train.shape[1]\n",
    "\n",
    "        \n",
    "        # コストや正答率の学習曲線を引くためのリストを用意\n",
    "        past_train_costs = []\n",
    "        past_test_costs = []\n",
    "        past_train_accuracy = []\n",
    "        past_test_accuracy = []\n",
    "        \n",
    "        # 初期化\n",
    "        # 重み初期化\n",
    "        # バッチサイズの設定\n",
    "        if self.params['batch_mode'] == 'batch':\n",
    "            self.params['batch_size'] = self.params['data_size']\n",
    "        elif self.params['batch_mode'] == 'mini':\n",
    "            self.params['batch_size'] = int(self.params['data_size']  * self.params['batch_size_rate'] ) \n",
    "        else:\n",
    "            self.params['batch_size'] = 1\n",
    "        # 隠れ層やレイヤーインスタンス生成\n",
    "        self.params['layer'] = Layers(self.params)\n",
    "        \n",
    "        \n",
    "        # 何イテレーションで1エポックか\n",
    "        epoch_per_i = int(self.params['data_size'] / self.params['batch_size'])\n",
    "        \n",
    "        \n",
    "        for i in range(self.params['iteration']):\n",
    "            \n",
    "            # 学習に使用するデータをサンプリング\n",
    "            choice_index = np.random.choice(self.params['data_size'], self.params['batch_size'])\n",
    "            X_batch, y_batch = X_train[choice_index], y_train[choice_index]\n",
    "            \n",
    "            # 誤差逆伝播法によって勾配を求め、値を更新\n",
    "            self.params['layer'].optimize(X_batch, y_batch)\n",
    "            \n",
    "            # 1エポックごとに正答率とコストを算出して保存する\n",
    "            if i % epoch_per_i == 0:              \n",
    "                past_train_accuracy.append(self.params['layer'].accuracy(X_train, y_train))\n",
    "                past_test_accuracy.append(self.params['layer'].accuracy(X_test, y_test))\n",
    "                \n",
    "                past_train_costs.append(self.params['layer'].loss(X_train, y_train))\n",
    "                past_test_costs.append(self.params['layer'].loss(X_test, y_test))\n",
    "            \n",
    "        return past_train_accuracy, past_test_accuracy, past_train_costs, past_test_costs \n",
    "    \n",
    "    def plot_learning_curve(self, X, y, metrics='acc', params={}): \n",
    "        past_train_accuracy, past_test_accuracy, past_train_costs, past_test_costs = self.train(X, y, params)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        # count_epoch = self.params['iteration'] // self.params['data_size'] + 1\n",
    "        if metrics == 'cost':\n",
    "            plt.plot(past_train_costs, color='orange', label='train')\n",
    "            plt.plot(past_test_costs, color='lime', label='test')\n",
    "            plt.ylabel(\"cost\", fontsize=15)\n",
    "            print(\"last train cost is {}\".format(past_train_costs[-1]))\n",
    "            print(\"last test cost is {}\".format(past_test_costs[-1]))\n",
    "        else:\n",
    "            #plt.plot(np.array(past_train_accuracy), color='r')\n",
    "            plt.plot(past_train_accuracy, color='orange', label='train')\n",
    "            plt.plot(past_test_accuracy, color='lime', label='test')\n",
    "            plt.ylabel(\"accuracy\", fontsize=15)\n",
    "            print(\"last train accuracy is {}\".format(past_train_accuracy[-1]))\n",
    "            print(\"last test accuracy is {}\".format(past_test_accuracy[-1]))\n",
    "            plt.ylim(-0.5, 100.5)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title('Learning Curve', fontsize=20)\n",
    "        plt.xlabel(\"iteration[epoch]\", fontsize=15)\n",
    "        \n",
    "    # 現在のパラメータで予測値を確率かラベルで出力する。\n",
    "    def predict(self, X, probability=False):\n",
    "        predict = self.params['layer'].predict(X)\n",
    "        predict_proba = softmax(predict)\n",
    "        if probability== True:\n",
    "            return predict_proba\n",
    "        else:\n",
    "            return np.argmax(predict_proba, axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルインスタンス生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(iteration=20000, optimizer='adam', \n",
    "            hidden_layer_list = [100, 100, 100, 100, 100, 100, 100], batch_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last train accuracy is 100.0\n",
      "last test accuracy is 97.19\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a223400b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {'optimizer': 'adam',\n",
    "         'batch_mode' : 'mini',\n",
    "         'init': 'he',\n",
    "         'lr': 0.007}\n",
    "model.plot_learning_curve(X,y, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle提出用ファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_sub = test_df\n",
    "X_sub = np.array(X_sub)\n",
    "\n",
    "Y_pred = model.predict(X_sub)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "       \"ImageId\": np.array(test_df.index) + 1,\n",
    "       \"Label\": Y_pred\n",
    "   })\n",
    "\n",
    "submission.to_csv(\"./submission_001.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle提出結果：0.97342"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
