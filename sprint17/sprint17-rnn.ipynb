{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIF_sprint17-rnn　RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN（リカレントニューラルネットワーク ）には時間（タイムステップ）の概念が導入されており、  \n",
    "手前の時刻の出力が後の時刻の入力として使用される構造を持つ。これによって、時系列上の特徴を  \n",
    "抽出することが可能となるため、文字列、音声、株価などの時系列データを扱うタスクに利用される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、IMDB映画レビューのデータセットを使用し、各レビューが肯定、否定のどちらであるかをRNNによって判定する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kerasによる実装   \n",
    "## simpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 17s 662us/step - loss: 0.5115 - acc: 0.7424 - val_loss: 0.4521 - val_acc: 0.7881\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 16s 631us/step - loss: 0.3098 - acc: 0.8711 - val_loss: 0.5151 - val_acc: 0.7573\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 16s 632us/step - loss: 0.1450 - acc: 0.9472 - val_loss: 0.6505 - val_acc: 0.7687\n",
      "25000/25000 [==============================] - 3s 118us/step\n",
      "Test loss: 0.6505090194892883\n",
      "Test accuracy: 0.76868\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 40\n",
    "batch_size = 32\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "inp = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features, 128)(inp) # max_featuresを128次元に成形\n",
    "simple_rnn_out = SimpleRNN(32)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(simple_rnn_out)\n",
    "model = Model(inputs=inp, outputs=predictions)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test))\n",
    "loss, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 31s 1ms/step - loss: 0.4798 - acc: 0.7608 - val_loss: 0.4127 - val_acc: 0.8090\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 29s 1ms/step - loss: 0.3294 - acc: 0.8556 - val_loss: 0.4123 - val_acc: 0.8102\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 31s 1ms/step - loss: 0.2396 - acc: 0.9017 - val_loss: 0.4631 - val_acc: 0.8004\n",
      "25000/25000 [==============================] - 4s 175us/step\n",
      "Test loss: 0.4630580368423462\n",
      "Test accuracy: 0.80036\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import SimpleRNN,GRU\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 40\n",
    "batch_size = 32\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "inp = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features, 128)(inp) # max_featuresを128次元に成形\n",
    "simple_rnn_out = GRU(32)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(simple_rnn_out)\n",
    "model = Model(inputs=inp, outputs=predictions)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test))\n",
    "loss, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 33s 1ms/step - loss: 0.4710 - acc: 0.7674 - val_loss: 0.4179 - val_acc: 0.8052\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 32s 1ms/step - loss: 0.3227 - acc: 0.8566 - val_loss: 0.4343 - val_acc: 0.8038\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 33s 1ms/step - loss: 0.2327 - acc: 0.9020 - val_loss: 0.4735 - val_acc: 0.7939\n",
      "25000/25000 [==============================] - 5s 196us/step\n",
      "Test loss: 0.4734947679901123\n",
      "Test accuracy: 0.79388\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import SimpleRNN,LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 40\n",
    "batch_size = 32\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "inp = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features, 128)(inp) # max_featuresを128次元に成形\n",
    "simple_rnn_out = LSTM(32)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(simple_rnn_out)\n",
    "model = Model(inputs=inp, outputs=predictions)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test))\n",
    "loss, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kerasの中間層の出力を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import SimpleRNN,LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 40\n",
    "batch_size = 32\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "inp = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "x = Embedding(max_features, 128)(inp) # max_featuresを128次元に成形\n",
    "simple_rnn_out = LSTM(32)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(simple_rnn_out)\n",
    "model = Model(inputs=inp, outputs=predictions)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "layers=[]\n",
    "get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "                                  [model.layers[1].output])\n",
    "layer_output = get_3rd_layer_output([x_train])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_2:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_8/GatherV2:0' shape=(?, 40, 128) dtype=float32>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.03416888,  0.03783664,  0.00586157, ..., -0.01624478,\n",
       "          0.00909306,  0.01922173],\n",
       "        [-0.04722911,  0.04446899,  0.04739446, ..., -0.00340547,\n",
       "         -0.04565128,  0.03332147],\n",
       "        [-0.02866079, -0.02254005,  0.02716302, ...,  0.00278345,\n",
       "          0.02685889,  0.01859263],\n",
       "        ...,\n",
       "        [ 0.03347677,  0.04959262,  0.00310228, ..., -0.00387122,\n",
       "          0.01852157, -0.04137981],\n",
       "        [-0.00931389,  0.00699956,  0.02615228, ...,  0.00101366,\n",
       "         -0.04573927,  0.03486884],\n",
       "        [ 0.01426223, -0.04787136,  0.03231819, ...,  0.03190733,\n",
       "         -0.02121609,  0.0231463 ]],\n",
       "\n",
       "       [[-0.00246792,  0.02797038,  0.03113807, ...,  0.02212362,\n",
       "          0.02466423,  0.04999024],\n",
       "        [ 0.02720055, -0.04736433,  0.01700575, ..., -0.0414017 ,\n",
       "         -0.007948  ,  0.00824443],\n",
       "        [ 0.03480778,  0.01760054,  0.01557399, ..., -0.01079752,\n",
       "         -0.04466461, -0.03789822],\n",
       "        ...,\n",
       "        [-0.01962675, -0.01568997,  0.00439792, ...,  0.02132517,\n",
       "          0.04091973,  0.01636548],\n",
       "        [-0.02957821,  0.03867346,  0.02630896, ...,  0.03199793,\n",
       "         -0.0226832 ,  0.0219327 ],\n",
       "        [-0.01132416,  0.04355022, -0.04185807, ..., -0.00543869,\n",
       "          0.02735176,  0.00489136]],\n",
       "\n",
       "       [[ 0.00666957,  0.03914908, -0.03920027, ...,  0.00296282,\n",
       "          0.00591277, -0.02589144],\n",
       "        [-0.03416888,  0.03783664,  0.00586157, ..., -0.01624478,\n",
       "          0.00909306,  0.01922173],\n",
       "        [-0.04102483, -0.00313181, -0.02640847, ...,  0.03134053,\n",
       "          0.0071831 , -0.03629999],\n",
       "        ...,\n",
       "        [ 0.01307892, -0.03392236,  0.02074974, ..., -0.02552941,\n",
       "          0.04479038, -0.04016318],\n",
       "        [-0.04772626, -0.00364821, -0.0457691 , ...,  0.00493066,\n",
       "          0.02456279, -0.01420058],\n",
       "        [ 0.0386325 ,  0.03922342,  0.02087401, ...,  0.03556185,\n",
       "         -0.03279189,  0.03977663]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.03967724,  0.01640279, -0.01006059, ...,  0.0215704 ,\n",
       "          0.04187726, -0.04227953],\n",
       "        [ 0.03182567,  0.04711225, -0.03727558, ..., -0.03168248,\n",
       "         -0.00793371, -0.03597157],\n",
       "        [ 0.03967724,  0.01640279, -0.01006059, ...,  0.0215704 ,\n",
       "          0.04187726, -0.04227953],\n",
       "        ...,\n",
       "        [ 0.04851515,  0.04023198, -0.04142625, ...,  0.01746322,\n",
       "          0.02339127, -0.03574188],\n",
       "        [-0.00711332,  0.01130165,  0.01222033, ...,  0.04980371,\n",
       "          0.01444497, -0.00771846],\n",
       "        [-0.02796391, -0.02394506,  0.01948399, ...,  0.01450223,\n",
       "         -0.03373746,  0.01869364]],\n",
       "\n",
       "       [[ 0.0393327 ,  0.03405331,  0.04518742, ..., -0.02634015,\n",
       "         -0.03703663,  0.01281256],\n",
       "        [ 0.03712804,  0.02751618, -0.03244257, ...,  0.00914   ,\n",
       "         -0.01863132, -0.04822019],\n",
       "        [-0.03849158,  0.04007056,  0.01367006, ...,  0.02921459,\n",
       "          0.01952944,  0.0023133 ],\n",
       "        ...,\n",
       "        [-0.04102483, -0.00313181, -0.02640847, ...,  0.03134053,\n",
       "          0.0071831 , -0.03629999],\n",
       "        [-0.03856494,  0.04462785, -0.01475651, ...,  0.02884972,\n",
       "          0.04334973,  0.00469489],\n",
       "        [ 0.00666957,  0.03914908, -0.03920027, ...,  0.00296282,\n",
       "          0.00591277, -0.02589144]],\n",
       "\n",
       "       [[ 0.03335393,  0.02458702,  0.04037077, ..., -0.01787261,\n",
       "         -0.02184813,  0.01092384],\n",
       "        [-0.02796391, -0.02394506,  0.01948399, ...,  0.01450223,\n",
       "         -0.03373746,  0.01869364],\n",
       "        [-0.00215162,  0.0077428 ,  0.02022593, ...,  0.00680577,\n",
       "         -0.01677157,  0.03841773],\n",
       "        ...,\n",
       "        [ 0.02354857,  0.04233951,  0.01474011, ...,  0.00358802,\n",
       "         -0.03786974,  0.04790083],\n",
       "        [-0.0454313 ,  0.04365117, -0.0354116 , ...,  0.01371426,\n",
       "          0.02410846, -0.03727324],\n",
       "        [-0.03856494,  0.04462785, -0.01475651, ...,  0.02884972,\n",
       "          0.04334973,  0.00469489]]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03416888,  0.03783664,  0.00586157, -0.02042237,  0.02980703,\n",
       "        0.04513042, -0.02026241,  0.01383116, -0.00808274,  0.04402683,\n",
       "        0.0141789 ,  0.02471377,  0.00486485, -0.02917048,  0.01988434,\n",
       "       -0.00414156, -0.00351591,  0.00242301,  0.01064868,  0.02924028,\n",
       "       -0.04600067,  0.00834206, -0.00471908,  0.02337712, -0.01399683,\n",
       "        0.04838243,  0.04735166,  0.00734   ,  0.01291228,  0.04309101,\n",
       "        0.03739926,  0.03233192, -0.0233871 ,  0.00374447,  0.00095449,\n",
       "        0.02878969,  0.02251926, -0.00867373, -0.02138217, -0.01424947,\n",
       "       -0.02910378, -0.03883444,  0.02741053, -0.0267715 , -0.01529577,\n",
       "        0.00185541,  0.030097  ,  0.0069525 , -0.01310914, -0.01322677,\n",
       "        0.00978652,  0.00373502,  0.01566556,  0.04557996,  0.02935301,\n",
       "        0.02947242, -0.00891559, -0.00899905, -0.01780238,  0.02641508,\n",
       "        0.01252932,  0.04723981, -0.01200227, -0.02359664, -0.00188351,\n",
       "       -0.0159466 , -0.03588001, -0.02022603, -0.00789155,  0.00492163,\n",
       "        0.0479955 ,  0.01370687, -0.03949487, -0.02299874, -0.03401866,\n",
       "        0.00122665, -0.02196428,  0.01884521, -0.01490812, -0.01536372,\n",
       "       -0.04167736,  0.04342106, -0.03092302, -0.01453205,  0.02977629,\n",
       "        0.04605374, -0.02354906,  0.0459126 , -0.01057833, -0.0109746 ,\n",
       "       -0.02311814,  0.00733728, -0.03298916,  0.01106638, -0.0301079 ,\n",
       "        0.03398533,  0.01460644,  0.02154838,  0.04492306,  0.03440661,\n",
       "        0.00818199, -0.00112356, -0.04327123, -0.01394925,  0.00387708,\n",
       "       -0.01445323, -0.00996856, -0.04275001, -0.02641056, -0.02698116,\n",
       "        0.00474391, -0.03564466, -0.00633423,  0.01093731,  0.04777894,\n",
       "       -0.03494401, -0.04887197, -0.04070952,  0.02565427, -0.00504571,\n",
       "       -0.03875464, -0.02019214,  0.01097953, -0.02609463,  0.01208354,\n",
       "       -0.01624478,  0.00909306,  0.01922173], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output[0,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chainer による実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read imdb\n",
      "constract vocabulary based on frequency\n",
      "# train data: 25000\n",
      "# test  data: 25000\n",
      "# vocab: 20000\n",
      "# class: 2\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           0.378264    0.0372162             0.993164       1                         178.308       \n",
      "\u001b[J2           0.00765625  0.00132426            1              1                         341.392       \n",
      "\u001b[J3           0.00112716  0.000803077           1              1                         502.959       \n",
      "\u001b[J"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "import chainer\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "import nets\n",
    "from nlp_utils import convert_seq\n",
    "import text_datasets\n",
    "\n",
    "batchsize = 32\n",
    "layer = 2\n",
    "unit = 32\n",
    "dropout = 0\n",
    "epoch = 3\n",
    "out = '/Users/h_t_mac_book_pro/NN/sprint17'\n",
    "\n",
    "def main():\n",
    "    current_datetime = '{}'.format(datetime.datetime.today())\n",
    "\n",
    "    train, test, vocab = text_datasets.get_imdb()\n",
    "    \n",
    "    print('# train data: {}'.format(len(train)))\n",
    "    print('# test  data: {}'.format(len(test)))\n",
    "    print('# vocab: {}'.format(len(vocab)))\n",
    "    n_class = len(set([int(d[1]) for d in train]))\n",
    "    print('# class: {}'.format(n_class))\n",
    "\n",
    "    train_iter = chainer.iterators.SerialIterator(train[:1000], batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test[:1000], batchsize,\n",
    "                                                 repeat=False, shuffle=False)\n",
    "    # インスタンス生成\n",
    "    Encoder = nets.RNNEncoder\n",
    "    encoder = Encoder(n_layers=layer, n_vocab=len(vocab),\n",
    "                      n_units=unit, dropout=dropout)\n",
    "    model = nets.TextClassifier(encoder, n_class)\n",
    "\n",
    "    # オプティマイザ\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(1e-4))\n",
    "\n",
    "    # トレーナー\n",
    "    updater = training.updaters.StandardUpdater(\n",
    "        train_iter, optimizer,\n",
    "        converter=convert_seq)\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "\n",
    "    # 検証\n",
    "    trainer.extend(extensions.Evaluator(\n",
    "                   test_iter, model, converter=convert_seq))\n",
    "\n",
    "    # ベストスコア記録\n",
    "    record_trigger = training.triggers.MaxValueTrigger(\n",
    "        'validation/main/accuracy', (1, 'epoch'))\n",
    "    trainer.extend(extensions.snapshot_object(\n",
    "        model, 'best_model.npz'),\n",
    "        trigger=record_trigger)\n",
    "\n",
    "    # loss , acc \n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "    \n",
    "    # 進捗バー表示\n",
    "    trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "    # 訓練実施\n",
    "    trainer.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スクラッチによるsimpleRNNの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ生成関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import SimpleRNN,LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def load_imdb(max_features = 10000, maxlen = 40,batch_size = 32):\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "    inp = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
    "    x = Embedding(max_features, 128)(inp) # max_featuresを128次元に成形\n",
    "    simple_rnn_out = LSTM(32)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(simple_rnn_out)\n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    layers=[]\n",
    "    get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "                                      [model.layers[1].output])\n",
    "    x_train = get_3rd_layer_output([x_train])[0]\n",
    "    x_test = get_3rd_layer_output([x_test])[0]\n",
    "    return x_train, y_train, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配チェック関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(forward, backvalue):\n",
    "    epsilon = 0.0001\n",
    "    gradient_checker = (forward(x + epsilon) -\n",
    "    forward(x - epsilon)) / (2 * epsilon)\n",
    "    diff = np.abs(backvalue - gradient_checker)\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 積\n",
    "class MultiplyGate:\n",
    "    def forward(self,W, x):\n",
    "        return np.dot(W, x)\n",
    "\n",
    "    def backward(self, W, x, dz):\n",
    "        dW = np.asarray(np.dot(dz, x.T))\n",
    "        dx = np.dot(np.transpose(W), dz)\n",
    "        return dW, dx\n",
    "    \n",
    "# 和\n",
    "class AddGate:\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2\n",
    "\n",
    "    def backward(self, x1, x2, dz):\n",
    "        dx1 = dz * np.ones_like(x1)\n",
    "        dx2 = dz * np.ones_like(x2)\n",
    "        return dx1, dx2\n",
    "    \n",
    "# アダマール\n",
    "class AdaGate:\n",
    "    def forward(self,x1, x2):\n",
    "        return x1 * x2\n",
    "    \n",
    "    def backward(self, x1, x2, dz):\n",
    "        dx1 = dz * x2\n",
    "        dx2 = dz * x1\n",
    "        return dx1, dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax(最終出力)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def predict(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores,axis=0)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[y])\n",
    "\n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        probs[y] -= 1.0\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh(活性化関数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, x, top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - np.square(output)) * top_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simpleRNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "class L:\n",
    "    def forward(self, x, prev_s, U, W, V):\n",
    "        self.mulu = mulGate.forward(U, x)\n",
    "        self.mulw = mulGate.forward(W, prev_s)\n",
    "        self.add = addGate.forward(self.mulw, self.mulu)\n",
    "        self.s = activation.forward(self.add)  \n",
    "        return self.add, self.s\n",
    "    \n",
    "    def backward(self, y, x, A, S, V, W, pred, num):\n",
    "        \n",
    "        diffs = pred - y\n",
    "        # dV\n",
    "        dV = np.dot(diffs , S[-1].T)\n",
    "        # dSt\n",
    "        pre_dSt = np.dot(V.T, diffs)\n",
    "        dU = 0\n",
    "        dW = 0\n",
    "        start = x.shape[1]-1\n",
    "        for i in range(start,start-num,-1):\n",
    "            #dAi\n",
    "            tanh = Tanh()\n",
    "            dA = tanh.backward(A[i], pre_dSt)\n",
    "            #dU\n",
    "            dU += np.dot(dA, x[:,i,:])\n",
    "            #dW\n",
    "            dW += np.dot(dA, S[i-1].T)\n",
    "            # dSi-1\n",
    "            pre_dSt = np.dot(W.T, dA)      \n",
    "            \n",
    "        return dV, dU, dW        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "class RNNmodels:\n",
    "    def __init__(self, word_dim, hidden_dim=100, truncate=4, batchsize=32, optimizer = 'sgd'):\n",
    "        self.optimizer = optimizer\n",
    "        self.batchsize = batchsize\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.truncate = truncate\n",
    "        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), \\\n",
    "                                   (hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim),\\\n",
    "                                   (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), \\\n",
    "                                   (2, hidden_dim))\n",
    "        if self.optimizer == 'adam':\n",
    "            self.m_U = np.zeros_like(self.U)\n",
    "            self.v_U = np.zeros_like(self.U)\n",
    "            self.m_W = np.zeros_like(self.W)\n",
    "            self.v_W = np.zeros_like(self.W)\n",
    "            self.m_V = np.zeros_like(self.V)\n",
    "            self.v_V = np.zeros_like(self.V)\n",
    "            self.beta1 = 0.88\n",
    "            self.beta2 = 0.998\n",
    "            self.adam_lr = 0.00095\n",
    "        else:\n",
    "            None\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "\n",
    "    def forward_propagation(self, x):   \n",
    "        T = x.shape[1]\n",
    "        self.all_A = []\n",
    "        self.all_S = []\n",
    "        prev_s = np.zeros((hidden_dim, x.shape[0]))\n",
    "        A = np.zeros((hidden_dim, x.shape[0]))\n",
    "        S = np.zeros((hidden_dim, x.shape[0]))\n",
    "        epsilon = 0.0001\n",
    "        dU_c = 0\n",
    "        dW_c = 0\n",
    "        dV_c = 0\n",
    "        for t in range(T):\n",
    "            layer = L()\n",
    "            A, S = layer.forward(x[:,t,:].T, prev_s, self.U, self.W, self.V)\n",
    "            prev_s = S\n",
    "            self.all_A.append(A)\n",
    "            self.all_S.append(S)     \n",
    "        return np.dot(self.V, S)\n",
    "            \n",
    "        \n",
    "    def loss_and_acc(self, x_train, y_train, x_test, y_test):\n",
    "        #forward\n",
    "        train_out = self.forward_propagation(x_train)\n",
    "        test_out = self.forward_propagation(x_test) \n",
    "        #predict\n",
    "        Soft = Softmax()\n",
    "        train_predict = Soft.predict(train_out)\n",
    "        test_predict = Soft.predict(test_out)\n",
    "        #acc\n",
    "        train_predict2 = train_predict.copy()\n",
    "        test_predict2 = test_predict.copy()\n",
    "        train_predict2[train_predict2<0.5]=0\n",
    "        test_predict2[test_predict2<0.5]=0\n",
    "        train_predict2[0.5<=train_predict2]=1\n",
    "        test_predict2[0.5<=test_predict2]=1\n",
    "        self.train_acc = (train_predict2 * y_train).sum() / y_train.shape[1]\n",
    "        self.test_acc = (test_predict2 * y_test).sum() / y_test.shape[1]\n",
    "        #loss\n",
    "        train_log = train_predict * y_train \n",
    "        self.train_loss = -np.log(train_log[np.where(1e-9<train_log)] +1e-8).sum() / y_train.shape[1]\n",
    "        test_log = test_predict * y_test    \n",
    "        self.test_loss = -np.log(test_log[np.where(1e-9<test_log)] +1e-8).sum() / y_test.shape[1]\n",
    "    \n",
    "\n",
    "    def update(self, dV, dU, dW, learning_rate):\n",
    "        if self.optimizer == 'sgd':\n",
    "            self.U -= learning_rate * dU \n",
    "            self.V -= learning_rate * dV \n",
    "            self.W -= learning_rate * dW \n",
    "            \n",
    "        elif self.optimizer == 'adam':\n",
    "            self.m_U = self.beta1 * self.m_U + (1- self.beta1) * dU\n",
    "            self.v_U = self.beta2 * self.v_U + (1- self.beta2) * (dU * dU)\n",
    "            m_hat_U = self.m_U / (1 - self.beta1)\n",
    "            v_hat_U = self.v_U / (1 - self.beta2)\n",
    "            self.U -= self.adam_lr * m_hat_U / (np.sqrt(v_hat_U) + 1e-8)\n",
    "            \n",
    "            self.m_V = self.beta1 * self.m_V + (1- self.beta1) * dV\n",
    "            self.v_V = self.beta2 * self.v_V + (1- self.beta2) * (dV * dV)\n",
    "            m_hat_V = self.m_V / (1 - self.beta1)\n",
    "            v_hat_V = self.v_V / (1 - self.beta2)\n",
    "            self.V -= self.adam_lr * m_hat_V / (np.sqrt(v_hat_V) + 1e-8)\n",
    "            \n",
    "            self.m_W = self.beta1 * self.m_W + (1- self.beta1) * dW\n",
    "            self.v_W = self.beta2 * self.v_W + (1- self.beta2) * (dW * dW)\n",
    "            m_hat_W = self.m_W / (1 - self.beta1)\n",
    "            v_hat_W = self.v_W / (1 - self.beta2)\n",
    "            self.W -= self.adam_lr * m_hat_W / (np.sqrt(v_hat_W) + 1e-8) \n",
    "        else:\n",
    "            None          \n",
    "\n",
    "            \n",
    "    def trains(self, Xtrain, Ytrain, Xtest, Ytest, learning_rate=0.005, nepoch=100):\n",
    "        iteration = len(Ytrain) // self.batchsize\n",
    "        y_hot = np.zeros((2,self.batchsize))  \n",
    "        pred1 = np.zeros((2,self.batchsize))\n",
    "        #Yrain ,Ytestをone-hot化しておく\n",
    "        Ytrain_hot = np.identity(2)[Ytrain].T\n",
    "        Ytest_hot = np.identity(2)[Ytest].T\n",
    "        Soft = Softmax()\n",
    "        for epoch in range(nepoch):\n",
    "            for itr in range(iteration):\n",
    "                start = itr * batchsize\n",
    "                x_batch = Xtrain[start:start + self.batchsize]\n",
    "                y_batch = Ytrain[start:start + self.batchsize]\n",
    "                y_hot = np.identity(2)[y_batch].T# (2, batchsize) 0行目が０ユニット用、1行目が１ユニット用\n",
    "                # forward\n",
    "                output = self.forward_propagation(x_batch)\n",
    "                #predict\n",
    "                pred1 = Soft.predict(output)\n",
    "                layers = L()\n",
    "                dV, dU, dW = layers.backward(y_hot, x_batch, self.all_A, self.all_S, self.V, \\\n",
    "                                             self.W, pred1, self.truncate)\n",
    "                \n",
    "                #update\n",
    "                self.update(dV, dU, dW, learning_rate)\n",
    "            \n",
    "            # loss\n",
    "            if (epoch==0) or ((epoch % 10) == 9):   \n",
    "                self.loss_and_acc(Xtrain, Ytrain_hot, Xtest, Ytest_hot)\n",
    "                print(\"*\"*50)\n",
    "                print(\"epoch\",epoch + 1)\n",
    "                print(\"train : loss {:.4} ,acc {:.4}\".format(self.train_loss, self.train_acc))\n",
    "                print(\"test : loss {:.4} ,acc {:.4}\".format(self.test_loss, self.test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset\n",
      "padding\n",
      "build model\n",
      "**************************************************\n",
      "epoch 1\n",
      "train : loss 0.6628 ,acc 0.6151\n",
      "test : loss 0.6677 ,acc 0.5893\n",
      "**************************************************\n",
      "epoch 10\n",
      "train : loss 0.6028 ,acc 0.667\n",
      "test : loss 0.6594 ,acc 0.6123\n",
      "**************************************************\n",
      "epoch 20\n",
      "train : loss 0.5963 ,acc 0.6724\n",
      "test : loss 0.6683 ,acc 0.6137\n",
      "**************************************************\n",
      "epoch 30\n",
      "train : loss 0.595 ,acc 0.6737\n",
      "test : loss 0.6737 ,acc 0.612\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "word_dim = 128\n",
    "hidden_dim = 64\n",
    "bptt_truncate = 4\n",
    "batchsize = 32\n",
    "train_size =7000\n",
    "test_size=3000\n",
    "learning_rate=0.002\n",
    "nepoch=30\n",
    "np.random.seed(15) #シード固定\n",
    "\n",
    "# データ取得\n",
    "x_train, y_train, x_test, y_test=load_imdb(max_features = 10000, maxlen = 40)\n",
    "#モデルインスタンス生成\n",
    "rnn = RNNmodels(word_dim, hidden_dim ,bptt_truncate, batchsize, optimizer='adam')\n",
    "#訓練・検証実施\n",
    "rnn.trains(x_train[:train_size], y_train[:train_size],x_test[:test_size], y_test[:test_size],\\\n",
    "           learning_rate, nepoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
